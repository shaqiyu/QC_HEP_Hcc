
[1mRooFit v3.60 -- Developed by Wouter Verkerke and David Kirkby[0m 
                Copyright (C) 2000-2013 NIKHEF, University of California & Stanford University
                All rights reserved, please read http://roofit.sourceforge.net/license.txt

number of signal: 2000 number of background: 2000
The data will be prepared for the quantum case so no transformation is needed.
Number of qubits used is 9 it should be the number of the variables used in the trainning.
Checking the number of qubits: 9
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
[[0, 1], [2, 3], [4, 5], [6, 7], [1, 2], [3, 4], [5, 6], [7, 8]]
Custom feature map:
      â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          
q_0: â”¤ H â”œâ”¤ Rz(2*x[0]) â”œâ”¤ Ry(x[0]) â”œâ”€â”€â– â”€â”€â”€â”€â”€â”€â”€
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”Œâ”€â”´â”€â”     
q_1: â”¤ H â”œâ”¤ Rz(2*x[1]) â”œâ”¤ Ry(x[1]) â”œâ”¤ X â”œâ”€â”€â– â”€â”€
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â””â”€â”€â”€â”˜â”Œâ”€â”´â”€â”
q_2: â”¤ H â”œâ”¤ Rz(2*x[2]) â”œâ”¤ Ry(x[2]) â”œâ”€â”€â– â”€â”€â”¤ X â”œ
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”Œâ”€â”´â”€â”â””â”€â”€â”€â”˜
q_3: â”¤ H â”œâ”¤ Rz(2*x[3]) â”œâ”¤ Ry(x[3]) â”œâ”¤ X â”œâ”€â”€â– â”€â”€
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â””â”€â”€â”€â”˜â”Œâ”€â”´â”€â”
q_4: â”¤ H â”œâ”¤ Rz(2*x[4]) â”œâ”¤ Ry(x[4]) â”œâ”€â”€â– â”€â”€â”¤ X â”œ
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”Œâ”€â”´â”€â”â””â”€â”€â”€â”˜
q_5: â”¤ H â”œâ”¤ Rz(2*x[5]) â”œâ”¤ Ry(x[5]) â”œâ”¤ X â”œâ”€â”€â– â”€â”€
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â””â”€â”€â”€â”˜â”Œâ”€â”´â”€â”
q_6: â”¤ H â”œâ”¤ Rz(2*x[6]) â”œâ”¤ Ry(x[6]) â”œâ”€â”€â– â”€â”€â”¤ X â”œ
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”Œâ”€â”´â”€â”â””â”€â”€â”€â”˜
q_7: â”¤ H â”œâ”¤ Rz(2*x[7]) â”œâ”¤ Ry(x[7]) â”œâ”¤ X â”œâ”€â”€â– â”€â”€
     â”œâ”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â””â”€â”€â”€â”˜â”Œâ”€â”´â”€â”
q_8: â”¤ H â”œâ”¤ Rz(2*x[8]) â”œâ”¤ Ry(x[8]) â”œâ”€â”€â”€â”€â”€â”¤ X â”œ
     â””â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”˜
# Tuning hyper-parameters for roc_auc

Best parameters set found on development set:

{'C': 1}

Grid scores on development set:

0.914 (+/-0.028) for {'C': 1}
0.904 (+/-0.037) for {'C': 2}
0.897 (+/-0.038) for {'C': 3}
0.891 (+/-0.040) for {'C': 4}
0.887 (+/-0.039) for {'C': 5}
0.882 (+/-0.039) for {'C': 6}
0.878 (+/-0.036) for {'C': 7}
0.874 (+/-0.035) for {'C': 8}
0.871 (+/-0.033) for {'C': 9}
0.868 (+/-0.033) for {'C': 10}
0.864 (+/-0.030) for {'C': 12}
0.861 (+/-0.029) for {'C': 14}
0.858 (+/-0.028) for {'C': 16}
0.855 (+/-0.025) for {'C': 18}
0.853 (+/-0.024) for {'C': 20}
0.851 (+/-0.024) for {'C': 22}
0.849 (+/-0.023) for {'C': 24}
0.847 (+/-0.021) for {'C': 26}
0.846 (+/-0.021) for {'C': 28}
0.845 (+/-0.020) for {'C': 30}
0.844 (+/-0.019) for {'C': 32}
0.842 (+/-0.019) for {'C': 34}
0.841 (+/-0.018) for {'C': 36}
0.840 (+/-0.018) for {'C': 38}
0.839 (+/-0.017) for {'C': 40}
0.837 (+/-0.015) for {'C': 50}

Detailed classification report:

The model is trained on the full development set.
The scores are computed on the full evaluation set.

              precision    recall  f1-score   support

        -1.0       0.86      0.81      0.83       492
         1.0       0.82      0.87      0.84       508

    accuracy                           0.84      1000
   macro avg       0.84      0.84      0.84      1000
weighted avg       0.84      0.84      0.84      1000


[0.         0.         0.         0.00203252 0.00203252 0.00406504
 0.00406504 0.00609756 0.00609756 0.00813008 0.00813008 0.0101626
 0.0101626  0.01219512 0.01219512 0.01422764 0.01422764 0.01829268
 0.01829268 0.0203252  0.0203252  0.02235772 0.02235772 0.02439024
 0.02439024 0.02642276 0.02642276 0.02845528 0.02845528 0.0304878
 0.0304878  0.03252033 0.03252033 0.03455285 0.03455285 0.03658537
 0.03658537 0.03861789 0.03861789 0.04065041 0.04065041 0.04268293
 0.04268293 0.04471545 0.04471545 0.04674797 0.04674797 0.04878049
 0.04878049 0.05081301 0.05081301 0.05487805 0.05487805 0.05691057
 0.05691057 0.05894309 0.05894309 0.06097561 0.06097561 0.06300813
 0.06300813 0.06504065 0.06504065 0.06707317 0.06707317 0.06910569
 0.06910569 0.07113821 0.07113821 0.07317073 0.07317073 0.07520325
 0.07520325 0.07723577 0.07723577 0.08130081 0.08130081 0.08333333
 0.08333333 0.08536585 0.08536585 0.08739837 0.08739837 0.08943089
 0.08943089 0.09552846 0.09552846 0.0995935  0.0995935  0.10162602
 0.10162602 0.10365854 0.10365854 0.10772358 0.10772358 0.1097561
 0.1097561  0.11178862 0.11178862 0.11585366 0.11585366 0.11788618
 0.11788618 0.1199187  0.1199187  0.12195122 0.12195122 0.12398374
 0.12398374 0.12601626 0.12601626 0.12804878 0.12804878 0.1300813
 0.1300813  0.13211382 0.13211382 0.13414634 0.13414634 0.13617886
 0.13617886 0.1504065  0.1504065  0.15243902 0.15243902 0.15447154
 0.15447154 0.16056911 0.16056911 0.16463415 0.16463415 0.16869919
 0.16869919 0.18089431 0.18089431 0.18292683 0.18292683 0.18902439
 0.18902439 0.19512195 0.19512195 0.19715447 0.19715447 0.20731707
 0.20731707 0.20934959 0.20934959 0.21138211 0.21138211 0.21544715
 0.21544715 0.2296748  0.2296748  0.23170732 0.23170732 0.23577236
 0.23577236 0.23780488 0.23780488 0.2398374  0.2398374  0.24390244
 0.24390244 0.24593496 0.24593496 0.25203252 0.25203252 0.27439024
 0.27439024 0.28252033 0.28252033 0.28658537 0.28658537 0.29878049
 0.29878049 0.30081301 0.30081301 0.30691057 0.30691057 0.34756098
 0.34756098 0.3495935  0.3495935  0.35365854 0.35365854 0.35569106
 0.35569106 0.36178862 0.36178862 0.3699187  0.3699187  0.38821138
 0.38821138 0.41260163 0.41260163 0.41666667 0.41666667 0.42479675
 0.42479675 0.42886179 0.42886179 0.43495935 0.43495935 0.4695122
 0.4695122  0.52235772 0.52235772 0.52439024 0.52439024 0.54471545
 0.54471545 0.56300813 0.56300813 0.61585366 0.61585366 0.65650407
 0.65650407 0.67479675 0.67479675 0.78658537 0.78658537 0.8800813
 0.8800813  0.91666667 0.91666667 1.        ]
[0.         0.0019685  0.02755906 0.02755906 0.07283465 0.07283465
 0.16141732 0.16141732 0.24212598 0.24212598 0.26377953 0.26377953
 0.26968504 0.26968504 0.2992126  0.2992126  0.3011811  0.3011811
 0.31889764 0.31889764 0.33070866 0.33070866 0.34448819 0.34448819
 0.38779528 0.38779528 0.39566929 0.39566929 0.3996063  0.3996063
 0.4015748  0.4015748  0.42125984 0.42125984 0.43700787 0.43700787
 0.4507874  0.4507874  0.47440945 0.47440945 0.52952756 0.52952756
 0.53346457 0.53346457 0.54527559 0.54527559 0.6023622  0.6023622
 0.60826772 0.60826772 0.61417323 0.61417323 0.61811024 0.61811024
 0.62007874 0.62007874 0.62992126 0.62992126 0.67125984 0.67125984
 0.67322835 0.67322835 0.68110236 0.68110236 0.68307087 0.68307087
 0.70275591 0.70275591 0.70866142 0.70866142 0.71062992 0.71062992
 0.71259843 0.71259843 0.71653543 0.71653543 0.71850394 0.71850394
 0.72244094 0.72244094 0.72440945 0.72440945 0.72834646 0.72834646
 0.73425197 0.73425197 0.73622047 0.73622047 0.74212598 0.74212598
 0.74409449 0.74409449 0.7480315  0.7480315  0.75393701 0.75393701
 0.75590551 0.75590551 0.76181102 0.76181102 0.77362205 0.77362205
 0.77559055 0.77559055 0.77755906 0.77755906 0.77952756 0.77952756
 0.78346457 0.78346457 0.80314961 0.80314961 0.80905512 0.80905512
 0.81299213 0.81299213 0.81889764 0.81889764 0.82677165 0.82677165
 0.82874016 0.82874016 0.83070866 0.83070866 0.84251969 0.84251969
 0.84645669 0.84645669 0.8484252  0.8484252  0.8523622  0.8523622
 0.85433071 0.85433071 0.85629921 0.85629921 0.86220472 0.86220472
 0.86811024 0.86811024 0.88188976 0.88188976 0.88385827 0.88385827
 0.88582677 0.88582677 0.88779528 0.88779528 0.88976378 0.88976378
 0.8996063  0.8996063  0.90354331 0.90354331 0.90551181 0.90551181
 0.91141732 0.91141732 0.91338583 0.91338583 0.91535433 0.91535433
 0.91732283 0.91732283 0.92125984 0.92125984 0.92913386 0.92913386
 0.93110236 0.93110236 0.93503937 0.93503937 0.93897638 0.93897638
 0.94488189 0.94488189 0.94685039 0.94685039 0.9507874  0.9507874
 0.95275591 0.95275591 0.95472441 0.95472441 0.95669291 0.95669291
 0.96062992 0.96062992 0.96259843 0.96259843 0.96456693 0.96456693
 0.96850394 0.96850394 0.97047244 0.97047244 0.97244094 0.97244094
 0.97440945 0.97440945 0.97637795 0.97637795 0.97834646 0.97834646
 0.98031496 0.98031496 0.98228346 0.98228346 0.98425197 0.98425197
 0.98622047 0.98622047 0.98818898 0.98818898 0.99015748 0.99015748
 0.99212598 0.99212598 0.99409449 0.99409449 0.99606299 0.99606299
 0.9980315  0.9980315  1.         1.        ]
[ 3.35744668  2.35744668  2.06012554  2.04677452  1.85732732  1.84709884
  1.63788363  1.6368231   1.50162978  1.50117878  1.47137726  1.47126278
  1.44635142  1.44631738  1.39427587  1.38480149  1.38224074  1.37827238
  1.33740244  1.33479234  1.32769951  1.32191748  1.30106843  1.30026144
  1.24308136  1.24260187  1.23377052  1.23219835  1.22682179  1.22396222
  1.22334957  1.21956698  1.18780919  1.18750164  1.16767751  1.16049611
  1.14792179  1.14777588  1.10297894  1.10191005  1.02449275  1.02062777
  1.0191185   1.01629907  0.99770649  0.9871788   0.8721696   0.86467317
  0.85720506  0.85566363  0.8548668   0.84640498  0.83988234  0.83864633
  0.83612168  0.83420323  0.82309614  0.82273543  0.73253871  0.72842869
  0.72657495  0.72358749  0.71497248  0.70642683  0.70462606  0.69130095
  0.65919735  0.65745152  0.64869845  0.64421486  0.64224632  0.63995467
  0.63765299  0.63485041  0.62586283  0.62273153  0.61907704  0.60276867
  0.59592885  0.5958393   0.58349733  0.58337677  0.57775403  0.57455859
  0.56741804  0.5467914   0.5416473   0.53662184  0.52410962  0.5225293
  0.51666925  0.51466362  0.50666844  0.49189956  0.47563472  0.46957517
  0.45843077  0.45811561  0.44491957  0.43733347  0.4021174   0.3997864
  0.39772884  0.39048443  0.38678856  0.38416358  0.38411747  0.37802139
  0.36284535  0.35827113  0.32160861  0.31659213  0.30031167  0.29429561
  0.28500633  0.28495155  0.26810253  0.25670992  0.21891083  0.21649794
  0.20926259  0.1754814   0.1643409   0.16160605  0.15191752  0.14109478
  0.13672486  0.12386541  0.12206026  0.10744452  0.08658235  0.0804324
  0.07868966  0.06275401  0.05662611  0.04080036  0.03023419  0.02771802
  0.0215795  -0.01011491 -0.04798876 -0.04852839 -0.04993549 -0.09048149
 -0.09704484 -0.10470329 -0.10527545 -0.10539739 -0.13107986 -0.13988263
 -0.17958727 -0.21280816 -0.22477372 -0.22672403 -0.23121516 -0.24531574
 -0.25049613 -0.2539409  -0.25626434 -0.26776844 -0.27782917 -0.28614489
 -0.29098571 -0.29351714 -0.29628281 -0.30399058 -0.31708294 -0.39490328
 -0.39565852 -0.42152137 -0.43278681 -0.44794537 -0.45707372 -0.49045423
 -0.4934287  -0.4947564  -0.49888311 -0.50362523 -0.51350957 -0.59408298
 -0.59634363 -0.61316652 -0.6252166  -0.62899654 -0.63095438 -0.63688779
 -0.64118429 -0.65071343 -0.65197019 -0.65721393 -0.66267516 -0.70048712
 -0.70846827 -0.74038641 -0.74156936 -0.74480719 -0.74871389 -0.75734611
 -0.75970996 -0.76320269 -0.76494229 -0.77316126 -0.77322024 -0.83386997
 -0.83569503 -0.9116967  -0.91329741 -0.91587619 -0.91675975 -0.93794501
 -0.93953088 -0.95800832 -0.95944552 -1.03877424 -1.04559601 -1.08800905
 -1.08873838 -1.11016523 -1.11036192 -1.23203873 -1.23634758 -1.39227578
 -1.40270945 -1.47297505 -1.47990569 -1.99843369]
**********************************************************
*******      Results for the simulator only        *******
**********************************************************
**********************************************************
